"""
Ù†Ø¸Ø§Ù… Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠ Ù„Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª PostgreSQL
=================================================
ØªØµØ¯ÙŠØ± Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙƒÙ…Ù„Ù SQL Ø£Ùˆ JSON
"""

import os
import subprocess
import json
import logging
from datetime import datetime
from dotenv import load_dotenv
import psycopg2
from psycopg2.extras import RealDictCursor

load_dotenv()
logger = logging.getLogger(__name__)

# PostgreSQL Configuration
POSTGRES_CONFIG = {
    'host': os.getenv('POSTGRES_HOST', 'localhost'),
    'port': os.getenv('POSTGRES_PORT', '5432'),
    'database': os.getenv('POSTGRES_DB', 'telegram_bot'),
    'user': os.getenv('POSTGRES_USER', 'bot_user'),
    'password': os.getenv('POSTGRES_PASSWORD')
}


def create_sql_backup():
    """
    Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© SQL Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… pg_dump
    
    Returns:
        tuple: (success: bool, file_path: str or error_message: str)
    """
    try:
        # Ø§Ø³Ù… Ù…Ù„Ù Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© Ù…Ø¹ Ø§Ù„ØªØ§Ø±ÙŠØ® ÙˆØ§Ù„ÙˆÙ‚Øª
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_filename = f"backup_postgres_{timestamp}.sql"
        backup_path = os.path.join(os.getcwd(), backup_filename)
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ Ø¨ÙŠØ¦Ø© Ø§Ù„ØªÙ†ÙÙŠØ° Ù…Ø¹ ÙƒÙ„Ù…Ø© Ø§Ù„Ù…Ø±ÙˆØ±
        env = os.environ.copy()
        env['PGPASSWORD'] = POSTGRES_CONFIG['password']
        
        # Ø¨Ù†Ø§Ø¡ Ø£Ù…Ø± pg_dump
        cmd = [
            'pg_dump',
            '-h', POSTGRES_CONFIG['host'],
            '-p', str(POSTGRES_CONFIG['port']),
            '-U', POSTGRES_CONFIG['user'],
            '-d', POSTGRES_CONFIG['database'],
            '-F', 'p',  # Plain text format
            '-f', backup_path
        ]
        
        logger.info(f"ğŸ”„ Ø¨Ø¯Ø¡ Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© SQL: {backup_filename}")
        
        # ØªÙ†ÙÙŠØ° Ø§Ù„Ø£Ù…Ø±
        result = subprocess.run(
            cmd,
            env=env,
            capture_output=True,
            text=True,
            timeout=60  # timeout Ø¨Ø¹Ø¯ Ø¯Ù‚ÙŠÙ‚Ø©
        )
        
        if result.returncode == 0:
            logger.info(f"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© Ø¨Ù†Ø¬Ø§Ø­: {backup_path}")
            return True, backup_path
        else:
            error_msg = result.stderr or "Ø®Ø·Ø£ ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ"
            logger.error(f"âŒ ÙØ´Ù„ pg_dump: {error_msg}")
            return False, f"pg_dump error: {error_msg}"
            
    except FileNotFoundError:
        logger.warning("âš ï¸ pg_dump ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ØŒ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø·Ø±ÙŠÙ‚Ø© JSON Ø§Ù„Ø¨Ø¯ÙŠÙ„Ø©")
        return False, "pg_dump not found"
    except subprocess.TimeoutExpired:
        logger.error("âŒ Ø§Ù†ØªÙ‡Øª Ù…Ù‡Ù„Ø© pg_dump")
        return False, "pg_dump timeout"
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ create_sql_backup: {e}")
        return False, str(e)


def create_json_backup():
    """
    Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© JSON (Ø·Ø±ÙŠÙ‚Ø© Ø¨Ø¯ÙŠÙ„Ø©)
    
    Returns:
        tuple: (success: bool, file_path: str or error_message: str)
    """
    try:
        # Ø§Ø³Ù… Ù…Ù„Ù Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_filename = f"backup_postgres_{timestamp}.json"
        backup_path = os.path.join(os.getcwd(), backup_filename)
        
        logger.info(f"ğŸ”„ Ø¨Ø¯Ø¡ Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© JSON: {backup_filename}")
        
        # Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        conn = psycopg2.connect(**POSTGRES_CONFIG)
        cursor = conn.cursor(cursor_factory=RealDictCursor)
        
        backup_data = {
            'backup_date': datetime.now().isoformat(),
            'database': POSTGRES_CONFIG['database'],
            'tables': {}
        }
        
        # Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ Ù†Ø³Ø®Ù‡Ø§
        tables = ['users', 'payments', 'settings', 'daily_downloads']
        
        for table_name in tables:
            try:
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¬Ø¯ÙˆÙ„
                cursor.execute(f'SELECT * FROM {table_name}')
                rows = cursor.fetchall()
                
                # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø© Ù…Ù† Ø§Ù„Ù‚ÙˆØ§Ù…ÙŠØ³
                backup_data['tables'][table_name] = [dict(row) for row in rows]
                
                logger.info(f"âœ… ØªÙ… Ù†Ø³Ø® Ø¬Ø¯ÙˆÙ„ {table_name}: {len(rows)} ØµÙ")
                
            except Exception as table_error:
                logger.warning(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ù†Ø³Ø® Ø¬Ø¯ÙˆÙ„ {table_name}: {table_error}")
                backup_data['tables'][table_name] = []
        
        conn.close()
        
        # Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙƒÙ…Ù„Ù JSON
        with open(backup_path, 'w', encoding='utf-8') as f:
            json.dump(backup_data, f, ensure_ascii=False, indent=2, default=str)
        
        logger.info(f"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© JSON Ø¨Ù†Ø¬Ø§Ø­: {backup_path}")
        return True, backup_path
        
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ create_json_backup: {e}")
        return False, str(e)


def create_backup(prefer_sql=True):
    """
    Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© (Ù…Ø­Ø§ÙˆÙ„Ø© SQL Ø£ÙˆÙ„Ø§Ù‹ØŒ Ø«Ù… JSON)
    
    Args:
        prefer_sql: ØªÙØ¶ÙŠÙ„ ØªÙ†Ø³ÙŠÙ‚ SQL Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ØªØ§Ø­Ø§Ù‹
    
    Returns:
        tuple: (success: bool, file_path: str or error_message: str)
    """
    if prefer_sql:
        success, result = create_sql_backup()
        if success:
            return success, result
        
        # Ø¥Ø°Ø§ ÙØ´Ù„ SQLØŒ Ù…Ø­Ø§ÙˆÙ„Ø© JSON
        logger.info("ğŸ”„ Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠ Ø¨ØªÙ†Ø³ÙŠÙ‚ JSON...")
        return create_json_backup()
    else:
        return create_json_backup()


def cleanup_old_backups(max_age_hours=24):
    """
    Ø­Ø°Ù Ù…Ù„ÙØ§Øª Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠ Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©
    
    Args:
        max_age_hours: Ø§Ù„Ø¹Ù…Ø± Ø§Ù„Ø£Ù‚ØµÙ‰ Ø¨Ø§Ù„Ø³Ø§Ø¹Ø§Øª Ù„Ù„Ù…Ù„ÙØ§Øª (Ø§ÙØªØ±Ø§Ø¶ÙŠ: 24 Ø³Ø§Ø¹Ø©)
    """
    try:
        import glob
        import time
        
        current_time = time.time()
        max_age_seconds = max_age_hours * 3600
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù…Ù„ÙØ§Øª Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠ
        backup_patterns = ['backup_postgres_*.sql', 'backup_postgres_*.json']
        
        deleted_count = 0
        for pattern in backup_patterns:
            for backup_file in glob.glob(pattern):
                file_age = current_time - os.path.getmtime(backup_file)
                
                if file_age > max_age_seconds:
                    try:
                        os.remove(backup_file)
                        logger.info(f"ğŸ—‘ï¸ ØªÙ… Ø­Ø°Ù Ù†Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© Ù‚Ø¯ÙŠÙ…Ø©: {backup_file}")
                        deleted_count += 1
                    except Exception as e:
                        logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø­Ø°Ù {backup_file}: {e}")
        
        if deleted_count > 0:
            logger.info(f"âœ… ØªÙ… Ø­Ø°Ù {deleted_count} Ù…Ù„Ù Ù†Ø³Ø® Ø§Ø­ØªÙŠØ§Ø·ÙŠ Ù‚Ø¯ÙŠÙ…")
            
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ cleanup_old_backups: {e}")


if __name__ == "__main__":
    # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ø¸Ø§Ù…
    logging.basicConfig(level=logging.INFO)
    
    print("ğŸ§ª Ø§Ø®ØªØ¨Ø§Ø± Ù†Ø¸Ø§Ù… Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠ...")
    success, result = create_backup()
    
    if success:
        print(f"âœ… Ù†Ø¬Ø­! Ø§Ù„Ù…Ù„Ù: {result}")
        print(f"ğŸ“¦ Ø­Ø¬Ù… Ø§Ù„Ù…Ù„Ù: {os.path.getsize(result) / 1024:.2f} KB")
    else:
        print(f"âŒ ÙØ´Ù„: {result}")
